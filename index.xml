<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Silas Bergen on Silas Bergen</title>
    <link>http://driftlessdata.space/</link>
    <description>Recent content in Silas Bergen on Silas Bergen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Silas Bergen</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0500</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A life of war (MakeoverMonday Week 6)</title>
      <link>http://driftlessdata.space/post/mm6-2020-uswar/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 -0600</pubDate>
      
      <guid>http://driftlessdata.space/post/mm6-2020-uswar/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://data.world/makeovermonday/2020w6&#34;&gt;This week&amp;rsquo;s MakeoverMonday&lt;/a&gt; is a good one.  Without further ado, the original visualization by Philip Bump, appearing in his &lt;a href=&#34;https://www.washingtonpost.com/politics/2020/01/08/nearly-quarter-americans-have-never-experienced-us-time-peace/&#34;&gt;Washington Post article&lt;/a&gt; entitled &lt;em&gt;Nearly
a quarter of Americans have never experienced the U.S. in a time of peace&lt;/em&gt;:&lt;/p&gt;

&lt;figure&gt; 
&lt;img src=&#34;http://driftlessdata.space/img/mm6-original.png&#34; alt=&#34;placeholder&#34; title=&#34;original graph&#34; height=&#34;400&#34; width=&#34;400&#34; /&gt;
&lt;/figure&gt;

&lt;p&gt;This graph triggered my pedagogical Pavlovian dog.  Not just because it&amp;rsquo;s easy to malign the poor pie chart (of which this graph has 115!), but because I had a hunch that a
redesign would reveal features of the data that are obscured above.&lt;/p&gt;

&lt;p&gt;But first, since we &lt;em&gt;do&lt;/em&gt; have pie charts, why not briefly discuss their relative merits and demerits.  Pie charts are fine if we want to show portions of a whole, and then
only if we have relatively few slices.  In each pie above, we have only two slices for most years. So a pie chart would be just fine if, for example, I only cared about
showing the relative portion of life lived in war and peace for people born in 1905.  The problem is that we don&amp;rsquo;t just have a pie for 1905, but for every year 1905-2019.  If we
wanted to see how the percents change over time, we have to compare slice sizes &lt;em&gt;across pies&lt;/em&gt;, and this is very hard to do perceptually.  This draws from the work of William Cleveland, who ranked the elementary perceptual tasks we use to
compare quantities.  We are very bad at making accurate comparisons when the quantities are encoded as angles (I discuss this in more detail &lt;a href=&#34;http://driftlessdata.space/post/on-epts/&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In the Washington Post graph, the thing that jumps out at me is the column of all-red on the right, and how much less red there is on the left.  It&amp;rsquo;s easy to see that people born since 2001 have lived their entire life
during a U.S. war; those born in the early 20th century lived a much smaller portion of their lives during war.  What is &lt;em&gt;much less&lt;/em&gt; obvious is the nature of the upward trend: was it monotone?  Up-and-down?
This is hard to tell!  It doesn&amp;rsquo;t help that in the original article, with my desktop Chrome browser at normal zoom, the graph is so tall that you have to scroll up and down to take it all in.&lt;/p&gt;

&lt;p&gt;Here is my redesign (&lt;a href=&#34;https://public.tableau.com/views/WhatportionofyourlifehastheU_S_beenatwarMakeoverMondayW6/Dashboard1?:display_count=y&amp;amp;publish=yes&amp;amp;:origin=viz_share_link&#34;&gt;Tableau Public link&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://driftlessdata.space/img/US_at_war.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I like to think of area charts as pie charts that work over time.  The redesign still shows us what the original did: people born in the early 20th century lived a much smaller portion of their lives
during war than people born in 2001.  We have preserved the &amp;ldquo;part-of-a-whole&amp;rdquo; story that pie charts have to tell. What the redesign reveals is that the upward trend is not monotone.  Rather we see a &amp;ldquo;sawtooth&amp;rdquo; pattern, where the saw teeth peak during wars
and and dip following wars.  Now that we have mapped these percents to a common vertical axis instead of encoded them as angles in a pie chart, we can perceive differences between them much more clearly.   And as a bonus, we don&amp;rsquo;t have to do any scrolling!&lt;/p&gt;

&lt;p&gt;A quick caveat: my redesign only includes the same wars listed in the original article, but you could argue the U.S. has been involved in many wars beyond just the ones shown.  Take a look for example at
&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_wars_involving_the_United_States&#34;&gt;this list from Wikipedia&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MakeoverMonday: Pesticide usage in US</title>
      <link>http://driftlessdata.space/post/makeovermondayweek2-pesticides/</link>
      <pubDate>Fri, 17 Jan 2020 00:00:00 -0600</pubDate>
      
      <guid>http://driftlessdata.space/post/makeovermondayweek2-pesticides/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s been a while since I&amp;rsquo;ve posted!  For a while I&amp;rsquo;ve been interested in joining the &lt;a href=&#34;https://www.makeovermonday.co.uk/&#34;&gt;MakeoverMonday&lt;/a&gt; community, a group
of data visualizers who come together each week to critique and redesign a visualization provided by Eva Murray and Andy Kriebel.  I haven&amp;rsquo;t, due to busyness mostly, but this semester I finally took the
dive and signed up!  Call it a 2020 resolution. I hope that this will become a regular opportunity for me to keep developing my visualization critique and design skills.&lt;/p&gt;

&lt;p&gt;I enjoyed my first challenge, &lt;a href=&#34;https://data.world/makeovermonday/2020w2&#34;&gt;MakeoverMonday 2020 Week 2&lt;/a&gt;,  a simple visualization that nonetheless provides a lot to think about and was a challenging redesign.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the original visualization, Figure 1 from the &lt;em&gt;Environmental Health&lt;/em&gt; article &lt;a href=&#34;https://ehjournal.biomedcentral.com/articles/10.1186/s12940-019-0488-0&#34;&gt;&lt;em&gt;The USA lags behind other agricultural nations in banning harmful pesticides&lt;/em&gt;:&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://driftlessdata.space/img/mm2-original.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://data.world/makeovermonday/2020w2/workspace/file?filename=MM+data+week+2+2020.xlsx&#34;&gt;data provided at data.world&lt;/a&gt; for the redesign don&amp;rsquo;t actually contain the same information on counts that we see in the original viz, but rather
data on total weight of pesticides that are banned/phased out in other countries that were applied used in the U.S. in 2016.&lt;/p&gt;

&lt;p&gt;My critiques of the original figure:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The way the &amp;ldquo;whole&amp;rdquo; is divvied up (whether that &amp;ldquo;whole&amp;rdquo; is number of pesticides or total lbs applied) between EU, Brazil, and China is very unclear.  The figure tries to get at intersections by
including $\geq 1$, $\geq 2$ and All 3, but it&amp;rsquo;s impossible to know which pesticides are banned by (for example) both China and Brazil.  It takes a while to understand what the &amp;ldquo;whole&amp;rdquo; is.&lt;/li&gt;
&lt;li&gt;The categories along the horizontal are a mix of both country labels and &lt;em&gt;number of countries&lt;/em&gt;.  I don&amp;rsquo;t like this mixing of label types.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Only counts of pesticides are shown: no information about &lt;em&gt;amount&lt;/em&gt; of pesticides applied is available in the visualization shown.  For example, China has banned only 11 pesticides, but does it ban the &amp;ldquo;big ones&amp;rdquo; as applied in the U.S.?  We can&amp;rsquo;t
tell from the figure.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In my redesign, I had a couple goals to address these critiques:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Make clear what the &amp;ldquo;whole&amp;rdquo; is.&lt;/li&gt;
&lt;li&gt;Allow the viewer to see where the intersections in banned pesticides exist.  E.g., if a pesticide is banned in the EU, is it also banned in China, or in Brazil?  All 3?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Visualization amount applied rather than counts.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To accomplish this I had to dig a bit deeper than the data provided on Data World, and found what I was looking for with &lt;a href=&#34;https://static-content.springer.com/esm/art%3A10.1186%2Fs12940-019-0488-0/MediaObjects/12940_2019_488_MOESM5_ESM.xlsx&#34;&gt;Additional File 5, Tables S131-S133&lt;/a&gt; from
the article.  I also had to do a bit of cleaning.  Here&amp;rsquo;s a &lt;a href=&#34;https://www.dropbox.com/s/oop1g8ws3dbwir3/pesticide_clean.csv?dl=0&#34;&gt;link to the data&lt;/a&gt; I ultimately ended up connecting to in Tableau&lt;/p&gt;

&lt;p&gt;Finally, my redesign (&lt;a href=&#34;https://public.tableau.com/views/pesticide_viz/Dashboard32?:display_count=y&amp;amp;publish=yes&amp;amp;:origin=viz_share_link&#34;&gt;Tableau Public version&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://driftlessdata.space/img/mm2.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Some design comments:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This accomplishes (I think) a clear visual of &amp;ldquo;the whole&amp;rdquo;: the giant (perhaps too giant) grey box with &amp;ldquo;328 million pounds&amp;rdquo; are the first things you see&lt;/li&gt;
&lt;li&gt;If desired, you can see which pesticides are banned in the EU, China, &lt;strong&gt;&lt;em&gt;and/or&lt;/em&gt;&lt;/strong&gt; Brazil.  This information was obscured before.&lt;/li&gt;
&lt;li&gt;You can now see that although China has banned fewer pesticides, they are pesticides that are more widely used (at least in the U.S.).&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;A critique of my own redesign: are the percents meaningful?  &amp;ldquo;98%&amp;rdquo; is a weird percent: the percent of &lt;em&gt;US-applied&lt;/em&gt; pesticides that are banned in the EU.  If they were banned in the US,
would they be replaced by something else?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some technical comments:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The grey box is giant, perhaps too much so, but getting Tableau to label the cells of the treemap in a satisfactory way was a pain.  Some of the labels (e.g. Fomesan) I added manually.&lt;/li&gt;
&lt;li&gt;Getting the four treemaps to divide the pesticides up in the same way, while applying different color schemes, was a pain and I used a hack approach (download the Tableau workbook if you are curious).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;R code for creating the .csv I used in my redesign, which begins with a &lt;a href=&#34;https://www.dropbox.com/s/npu4pp5oaxewznz/pesticide_complete_stack.csv?dl=0&#34;&gt;stacked version&lt;/a&gt; of Tables S131-S133 from
&lt;a href=&#34;https://static-content.springer.com/esm/art%3A10.1186%2Fs12940-019-0488-0/MediaObjects/12940_2019_488_MOESM5_ESM.xlsx&#34;&gt;Additional file 5&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pest &amp;lt;- read.csv(&#39;pesticide_complete_stack.csv&#39;)

library(tidyr) 
library(dplyr)
lbs &amp;lt;- pest %&amp;gt;% group_by(Pesticide) %&amp;gt;% summarize(USLbs2016 = mean(USLbs2016))
use &amp;lt;- function(col) ifelse(is.na(col),0,1)

out &amp;lt;- pest %&amp;gt;% spread(key = CountryBanned, value = USLbs2016) %&amp;gt;%
  inner_join(lbs, by = &#39;Pesticide&#39;) %&amp;gt;%
  mutate_at(vars(EU:Brazil), use) %&amp;gt;%
  mutate(nbanned = Brazil + China + EU) %&amp;gt;%
  mutate(used_US = ifelse(USLbs2016&amp;gt; 0, 1, 0)) %&amp;gt;% 
  select(Pesticide, USLbs2016, EU,China,Brazil,nbanned,used_US)

write.csv(out, file = &#39;pesticide_clean.csv&#39;,row.names=FALSE)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Data Visualization: Principles and Applications in R, Tableau, and Python</title>
      <link>http://driftlessdata.space/project/sdss19-workshop/</link>
      <pubDate>Sun, 02 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>http://driftlessdata.space/project/sdss19-workshop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A core data science curriculum for undergraduates</title>
      <link>http://driftlessdata.space/project/uscots2019-workshop/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      
      <guid>http://driftlessdata.space/project/uscots2019-workshop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A word in favor of summarytools</title>
      <link>http://driftlessdata.space/post/summarytools/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://driftlessdata.space/post/summarytools/</guid>
      <description>&lt;p&gt;Yesterday, I was preparing material for STAT 405 (biostatistics) I am teaching this spring, and was on the prowl for something that is an improvement upon the base R &lt;code&gt;summary()&lt;/code&gt; function (it doesn’t even give standard deviations!). The ideal package would also improve upon the base R &lt;code&gt;table()&lt;/code&gt; method, for which getting row and/or column percents is a huge pain. Base function &lt;code&gt;xtabs()&lt;/code&gt; is great for getting arrays of contigency tables, but no percents. My first stop was the &lt;a href=&#34;https://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf&#34;&gt;&lt;code&gt;Hmisc&lt;/code&gt; package&lt;/a&gt;, which has a good summary method via its &lt;code&gt;describe()&lt;/code&gt; function.&lt;br /&gt;
To demonstrate I use the &lt;a href=&#34;http://www.biostat.ucsf.edu/vgsm/data.html&#34;&gt;Western Collaborative Group Survey (WCGS)&lt;/a&gt; data from Eric Vittingoff’s excellent book &lt;em&gt;Regression Methods in Biostatistics&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Hmisc::describe(wcgs[,1:5])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## wcgs[, 1:5] 
## 
##  5  Variables      3154  Observations
## ---------------------------------------------------------------------------
## age 
##        n  missing distinct     Info     Mean      Gmd      .05      .10 
##     3154        0       21    0.996    46.28    6.256       39       40 
##      .25      .50      .75      .90      .95 
##       42       45       50       55       57 
## 
## lowest : 39 40 41 42 43, highest: 55 56 57 58 59
## ---------------------------------------------------------------------------
## arcus 
##        n  missing distinct     Info      Sum     Mean      Gmd 
##     3152        2        2    0.628      941   0.2985    0.419 
## 
## ---------------------------------------------------------------------------
## behpat 
##        n  missing distinct 
##     3154        0        4 
##                                   
## Value         A1    A2    B3    B4
## Frequency    264  1325  1216   349
## Proportion 0.084 0.420 0.386 0.111
## ---------------------------------------------------------------------------
## bmi 
##        n  missing distinct     Info     Mean      Gmd      .05      .10 
##     3154        0      679        1    24.52    2.803    20.59    21.52 
##      .25      .50      .75      .90      .95 
##    22.96    24.39    25.84    27.45    28.73 
## 
## lowest : 11.19061 15.66050 16.87200 17.21633 17.22242
## highest: 36.04248 37.22973 37.24805 37.65281 38.94737
## ---------------------------------------------------------------------------
## chd69 
##        n  missing distinct 
##     3154        0        2 
##                       
## Value         No   Yes
## Frequency   2897   257
## Proportion 0.919 0.081
## ---------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It also has a &lt;code&gt;summary&lt;/code&gt; method for objects of class &lt;code&gt;formula&lt;/code&gt; which ultimately can be used to create tables that are ready for markdown:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(chd69~agec, data = wcgs,method=&amp;#39;reverse&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 
## Descriptive Statistics by chd69
## 
## +------------+-----------+-----------+
## |            |No         |Yes        |
## |            |(N=2897)   |(N=257)    |
## +------------+-----------+-----------+
## |agec : 35-40|18%  ( 512)|12%  (  31)|
## +------------+-----------+-----------+
## |    41-45   |36%  (1036)|21%  (  55)|
## +------------+-----------+-----------+
## |    46-50   |23%  ( 680)|27%  (  70)|
## +------------+-----------+-----------+
## |    51-55   |16%  ( 463)|25%  (  65)|
## +------------+-----------+-----------+
## |    56-60   | 7%  ( 206)|14%  (  36)|
## +------------+-----------+-----------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It does not work as well as you would expect for additional dimensions, however:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(chd69~agec+behpat, data = wcgs,method=&amp;#39;reverse&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 
## Descriptive Statistics by chd69
## 
## +------------+-----------+-----------+
## |            |No         |Yes        |
## |            |(N=2897)   |(N=257)    |
## +------------+-----------+-----------+
## |agec : 35-40|18%  ( 512)|12%  (  31)|
## +------------+-----------+-----------+
## |    41-45   |36%  (1036)|21%  (  55)|
## +------------+-----------+-----------+
## |    46-50   |23%  ( 680)|27%  (  70)|
## +------------+-----------+-----------+
## |    51-55   |16%  ( 463)|25%  (  65)|
## +------------+-----------+-----------+
## |    56-60   | 7%  ( 206)|14%  (  36)|
## +------------+-----------+-----------+
## |behpat : A1 | 8%  ( 234)|12%  (  30)|
## +------------+-----------+-----------+
## |    A2      |41%  (1177)|58%  ( 148)|
## +------------+-----------+-----------+
## |    B3      |40%  (1155)|24%  (  61)|
## +------------+-----------+-----------+
## |    B4      |11%  ( 331)| 7%  (  18)|
## +------------+-----------+-----------+&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Enter &lt;a href=&#34;https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html&#34;&gt;summarytools&lt;/a&gt;, immediately appealing in its simplicity. Indeed it only has four primary functions, centered on its wonderful &lt;code&gt;dfSummary()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(summarytools)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../img/dfsummary1.PNG&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;This is the prettiest, most thorough output I’ve come across in a summary function, complete with ASCII bar graphs or histograms representing categorical or quantitative variables. You can prettify it even further in the browser with the &lt;code&gt;view()&lt;/code&gt; command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;view(dfSummary(wcgs))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../img/dfsummary2.PNG&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Two-way tables come by way of &lt;code&gt;ctable()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ctable(wcgs$agec,wcgs$chd69)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Cross-Tabulation / Row Proportions   
## Variables: agec * chd69     
## Data Frame: wcgs   
##    
## ------- ------- --------------- -------------- ----------------
##           chd69              No            Yes            Total
##    agec                                                        
##   35-40            512 (94.29%)    31 ( 5.71%)    543 (100.00%)
##   41-45           1036 (94.96%)    55 ( 5.04%)   1091 (100.00%)
##   46-50            680 (90.67%)    70 ( 9.33%)    750 (100.00%)
##   51-55            463 (87.69%)    65 (12.31%)    528 (100.00%)
##   56-60            206 (85.12%)    36 (14.88%)    242 (100.00%)
##   Total           2897 (91.85%)   257 ( 8.15%)   3154 (100.00%)
## ------- ------- --------------- -------------- ----------------&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A couple drawbacks to &lt;code&gt;summarytools&lt;/code&gt;: (1) It is not very compatible with the tidyverse, as you can see with the above use of &lt;code&gt;ctable()&lt;/code&gt;. (2) Even the nice 2x2 table is not easily extentable to higher dimensions. You could use &lt;code&gt;by()&lt;/code&gt;, but…who wants to do that?&lt;/p&gt;
&lt;p&gt;In finishing this post I see Adam Medcalf from Dabbling with Data has a &lt;a href=&#34;https://dabblingwithdata.wordpress.com/2018/01/02/my-favourite-r-package-for-summarising-data/&#34;&gt;nice post&lt;/a&gt; on Hmisc, summarytools, and a couple others as well. For my money, I’ll take summarytools, though I wish its beautiful 2x2 table displays were more easily extended and its &lt;code&gt;ctable()&lt;/code&gt; and &lt;code&gt;descr()&lt;/code&gt; functions more tidyverseable!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stat consulting: a data science playground</title>
      <link>http://driftlessdata.space/post/dsci-consulting/</link>
      <pubDate>Wed, 02 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://driftlessdata.space/post/dsci-consulting/</guid>
      <description>&lt;p&gt;This past semester I taught our &lt;a href=&#34;../../courses/stat370/stat370-home/&#34;&gt;STAT 370 (statistical consulting and communication)&lt;/a&gt; for the first time. This course gave student experience consulting for real clients from the university and community and focused on communicating with a client as well as report and presentation preparation best practices. Most of the required analyses were simple: paired t-tests, simple linear regression, etc. What struck me was the nontrivality of the data tidying process! While STAT 370 is taken mostly by our statistics majors, so many of the examples we encountered would be beautiful case studies for our introductory DSCI (data science) curriculum. In this post I present an actual example from a client that illustrates this.&lt;/p&gt;
&lt;p&gt;The data here concerned undergraduate nursing students in one of four terms of the nursing program. Of interest was measuring the students’ resiliency as measured by the Connor-Davidson Resiliency Scale (CDRS), both prior to and following impelementation of a Stress Management and Resiliency Training (SMART). The client was interested in determining for which of the four terms was there a significant change in resilience.&lt;/p&gt;
&lt;p&gt;Here are the data we’re working with (&lt;a href=&#34;../../files/cdrs_data_pre.csv&#34;&gt;link to pre&lt;/a&gt; | &lt;a href=&#34;../../files/cdrs_data_post.csv&#34;&gt;link to post&lt;/a&gt;). &lt;code&gt;id&lt;/code&gt; stands for a unique student identifier. We also have responses to 18 of the CDRS items (each on a 5-point Likert scale). The &lt;code&gt;post&lt;/code&gt; data set also contains the student terms; notably this information is &lt;em&gt;not&lt;/em&gt; available in the &lt;code&gt;pre&lt;/code&gt; data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(ggplot2)
pre &amp;lt;- read.csv(&amp;#39;cdrs_data_pre.csv&amp;#39;)
post &amp;lt;- read.csv(&amp;#39;cdrs_data_post.csv&amp;#39;)
head(post)
names(pre) #missing term information!!&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id  Term CDRS_Q1 CDRS_Q2 CDRS_Q3 CDRS_Q4 CDRS_Q5 CDRS_Q6 CDRS_Q7
## 1  23 Term1       4       4       3       4       4       3       3
## 2 183 Term1       3       4       4       3       4       3       4
## 3  80 Term1       3       4       4       3       4       3       4
## 4   1 Term1       3       4       4       3       3       2       4
## 5 166 Term1       3       4       3       4       4       4       4
## 6  15 Term1       3       3       2       3       3       2       3
##   CDRS_Q8 CDRS_Q9 CDRS_Q10 CDRS_Q11 CDRS_Q12 CDRS_Q20 CDRS_Q21 CDRS_Q22
## 1       4       4        4        4        3        2        3        3
## 2       2       3        3        3        3        3        3        3
## 3       3       4        4        4        4        4        4        4
## 4       4       4        4        4        4        2        3        3
## 5       4       4        4        4        4        3        4        4
## 6       3       2        3        3        3        2        3        3
##   CDRS_Q23 CDRS_Q24 CDRS_Q25
## 1        3        4        4
## 2        3        4        4
## 3        3        4        4
## 4        2        3        4
## 5        4        4        4
## 6        3        3        3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;id&amp;quot;       &amp;quot;CDRS_Q1&amp;quot;  &amp;quot;CDRS_Q2&amp;quot;  &amp;quot;CDRS_Q3&amp;quot;  &amp;quot;CDRS_Q4&amp;quot;  &amp;quot;CDRS_Q5&amp;quot; 
##  [7] &amp;quot;CDRS_Q6&amp;quot;  &amp;quot;CDRS_Q7&amp;quot;  &amp;quot;CDRS_Q8&amp;quot;  &amp;quot;CDRS_Q9&amp;quot;  &amp;quot;CDRS_Q10&amp;quot; &amp;quot;CDRS_Q11&amp;quot;
## [13] &amp;quot;CDRS_Q12&amp;quot; &amp;quot;CDRS_Q20&amp;quot; &amp;quot;CDRS_Q21&amp;quot; &amp;quot;CDRS_Q22&amp;quot; &amp;quot;CDRS_Q23&amp;quot; &amp;quot;CDRS_Q24&amp;quot;
## [19] &amp;quot;CDRS_Q25&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the data are not &lt;a href=&#34;https://www.jstatsoft.org/article/view/v059i10&#34;&gt;tidy&lt;/a&gt; in the sense that each row is a person, and we have variable information on the questions in columns. I’ll return to this in a bit.&lt;/p&gt;
&lt;p&gt;Here ultimately is a visualization that we could use to determine for which terms are the SMART effects strongest, and for which terms is the effect statistically significant:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://driftlessdata.space/post/dsci-consulting_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the average resilience pre-SMART was lower than average resilience post-SMART, and that these differences were most extreme for students in Terms 2 and 3 (which were also the only statistically significant differences). Additionally, students in Term 4 had very high pre- and post-SMART resilience (they’re seasoned veterans, after all!)&lt;/p&gt;
&lt;p&gt;A simple plot, with a simple interpretation. But the path to get there is anything but! To create this plot we need:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;to average the 18 CDRS items for each student;&lt;/li&gt;
&lt;li&gt;join the data sets;&lt;/li&gt;
&lt;li&gt;compute paired t-tests for each term;&lt;/li&gt;
&lt;li&gt;prepare data for plotting;&lt;/li&gt;
&lt;li&gt;plot&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, let’s proceed!&lt;/p&gt;
&lt;div id=&#34;average-the-cdrs-items&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Average the CDRS items&lt;/h2&gt;
&lt;p&gt;This is perhaps the most interesting step in the process. As mentioned earlier, the data are not tidy in the sense that we have variable information in columns instead of rows. We could reshape (“gather” or “melt”) to average CDRS score by term. Doing this for the post data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_melt &amp;lt;- post %&amp;gt;% 
  gather(key = &amp;#39;Question&amp;#39;,value=&amp;#39;Response&amp;#39;,CDRS_Q1:CDRS_Q25)
head(post_melt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id  Term Question Response
## 1  23 Term1  CDRS_Q1        4
## 2 183 Term1  CDRS_Q1        3
## 3  80 Term1  CDRS_Q1        3
## 4   1 Term1  CDRS_Q1        3
## 5 166 Term1  CDRS_Q1        3
## 6  15 Term1  CDRS_Q1        3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_melt %&amp;gt;% 
  group_by(id,Term) %&amp;gt;%
  summarize(post_mean = mean(Response))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 67 x 3
## # Groups:   id [?]
##       id   Term post_mean
##    &amp;lt;int&amp;gt; &amp;lt;fctr&amp;gt;     &amp;lt;dbl&amp;gt;
##  1     1  Term1  3.333333
##  2     2  Term3  3.000000
##  3     4  Term2  2.833333
##  4     8  Term2  3.111111
##  5     9  Term1  2.722222
##  6    13  Term4  2.722222
##  7    15  Term1  2.777778
##  8    20  Term1  3.000000
##  9    23  Term1  3.500000
## 10    26  Term4  2.555556
## # ... with 57 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also forego the melting, since ultimately all we want is the average response for each student. We can use the original &lt;code&gt;post&lt;/code&gt; data set, and an application of the &lt;code&gt;rowMeans()&lt;/code&gt; function within which is nested the &lt;code&gt;select()&lt;/code&gt; command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post2 &amp;lt;- post %&amp;gt;%
  mutate(post_mean = rowMeans(select(.,CDRS_Q1:CDRS_Q25))) %&amp;gt;%
  select(id, Term, post_mean)
head(post2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id  Term post_mean
## 1  23 Term1  3.500000
## 2 183 Term1  3.277778
## 3  80 Term1  3.722222
## 4   1 Term1  3.333333
## 5 166 Term1  3.833333
## 6  15 Term1  2.777778&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One could argue that the first approach (using &lt;code&gt;gather()&lt;/code&gt;) is the best, since it first creates a “tidy” data set and is arguably more readable. But the second appears more succinct. I’ll use the second approach to carry out the same task on the &lt;code&gt;pre&lt;/code&gt; data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pre2 &amp;lt;- pre %&amp;gt;%
  mutate(pre_mean = rowMeans(select(.,CDRS_Q1:CDRS_Q25))) %&amp;gt;%
  select(id, pre_mean)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;join-pre-and-post&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Join pre and post&lt;/h2&gt;
&lt;p&gt;This step is easy!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;both &amp;lt;- inner_join(pre2,post2,by=&amp;#39;id&amp;#39;)
head(both)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id pre_mean  Term post_mean
## 1  1 3.333333 Term1  3.333333
## 2  2 2.944444 Term3  3.000000
## 3  4 2.888889 Term2  2.833333
## 4  8 2.944444 Term2  3.111111
## 5  9 2.444444 Term1  2.722222
## 6 13 2.944444 Term4  2.722222&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;carry-out-paired-t-tests-by-term&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Carry out paired t-tests by term&lt;/h2&gt;
&lt;p&gt;Here is the lone “formal” statistical aspect of the whole problem! We can carry out paired t-tests by term as follows using some ugly base-R code: the &lt;code&gt;by()&lt;/code&gt; command. I won’t show the output, but here we can see that only for Terms 2 and 3 is the SMART effect statistically significant:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;by(both, both$Term, function(df) t.test(df$pre_mean,df$post_mean,paired=TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;prepare-data-for-plotting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prepare data for plotting&lt;/h2&gt;
&lt;p&gt;Referring back to the figure, the geometric mapping includes four points for the four pre-SMART CDRS averages (one for each term); four points for the post-SMART CDRS averages (one for each term); and a line segment connecting them. We can use the joined data set to form our “plotting” data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;toplot &amp;lt;- both %&amp;gt;%
  group_by(Term) %&amp;gt;%
  summarize(avg_pre = mean(pre_mean),avg_post = mean(post_mean)) %&amp;gt;% 
  mutate(sig = c(&amp;#39;no&amp;#39;,&amp;#39;yes&amp;#39;,&amp;#39;yes&amp;#39;,&amp;#39;no&amp;#39;))
toplot&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
##     Term  avg_pre avg_post   sig
##   &amp;lt;fctr&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1  Term1 2.965608 3.100529    no
## 2  Term2 2.977778 3.161111   yes
## 3  Term3 2.952675 3.255144   yes
## 4  Term4 3.211111 3.200000    no&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;avg_pre&lt;/code&gt; and &lt;code&gt;avg_post&lt;/code&gt; columns are in fact “means of means,” and we have manually added a column to indicate whether the difference was statistically significant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot!&lt;/h2&gt;
&lt;p&gt;And now the fun begins! Here is ggplot code to create the original figure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = toplot) + 
  geom_point(aes(x = avg_pre, y = 1:4, shape=&amp;#39;a&amp;#39;,color=sig),size=2)  + 
  geom_point(aes(x = avg_post, y = 1:4,shape=&amp;#39;b&amp;#39;,color=sig),size=2) + 
  geom_segment(aes(x = avg_pre, xend = avg_post, y=1:4,yend=1:4,color=sig)) + 
  scale_y_reverse() + xlab(&amp;#39;average CDRS&amp;#39;) + ylab(&amp;#39;Term&amp;#39;) + 
  scale_shape_manual(name=&amp;#39;&amp;#39;,values=c(&amp;#39;a&amp;#39;=19,&amp;#39;b&amp;#39;=17),labels=c(&amp;#39;pre SMART&amp;#39;,&amp;#39;post SMART&amp;#39;)) + 
  scale_color_discrete(name=&amp;#39;significant?&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://driftlessdata.space/post/dsci-consulting_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;moral&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Moral&lt;/h2&gt;
&lt;p&gt;Wow! The only truly “statistical” aspect of this whole process was a paired t-test. And even that was a bit tricky: figuring out how to carry out these t-tests by term! In reflecting back on the semester, I am struck that our data science students would do well to take our stat consulting course. They would be kept very interested in data “wrangling” tasks such as this. On the flip side, it’s an invaluable experience for our STAT majors (who comprise the usual audience of this course) to realize that the formal statistical analysis in which they are most trained is ultimately the last in a long series of data wrangling steps.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data vizzing in Japan</title>
      <link>http://driftlessdata.space/post/icots/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 -0500</pubDate>
      
      <guid>http://driftlessdata.space/post/icots/</guid>
      <description>&lt;p&gt;This July, my colleage Todd Iverson and I had the incredible opportunity to lead a &lt;a href=&#34;https://icots.info/10/?workshop=E&#34;&gt;pre-conference workshop&lt;/a&gt; at &lt;a href=&#34;https://icots.info/10/&#34;&gt;ICOTS 10&lt;/a&gt; in Kyoto, Japan.  Our workshop was titled &lt;em&gt;Data visualization: best practices and principles using Tableau Public and Python.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Our workshop began by covering &lt;a href=&#34;https://www.springer.com/us/book/9780387245447&#34;&gt;Leland Wilkinson&amp;rsquo;s grammar of graphics&lt;/a&gt;.  Most data visualization software (Tableau, Python, R, JMP) employ some version of this grammar, and with a firm understanding it becomes easy to transition between them.  We focused primarily on Tableau and Python in our workshop.  All the workshop materials are available at the designated &lt;a href=&#34;https://github.com/WSU-DataScience/ICOTS10_Data_Visualization&#34;&gt;Github repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It was an engaging four hours that felt more like one hour, with participants from all over the world!  Our fantastic crew:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
 &lt;figure&gt;
    &lt;img width=&#34;500&#34; src=&#34;http://driftlessdata.space/img/icots.jpg&#34;&gt;
 &lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;ICOTS 10 itself was fantastic, including keynotes from Chris Wild, Hillary Parker, and Anna Rosling R&amp;ouml;nnlund. And of course, we made time to explore and sample the local cuisine.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://driftlessdata.space/img/fushimi.jpg&#34;  style=&#34;float: left; width: 48%; margin-right: 1%; margin-bottom: 0.5em;&#34;&gt;
&lt;img src=&#34;http://driftlessdata.space/img/ramen.jpg&#34;  style=&#34;float: left; width: 48%; margin-right: 1%; margin-bottom: 0.5em;&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;p style=&#34;clear: both;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data visualization: Principles and Practice with Tableau Public and Python</title>
      <link>http://driftlessdata.space/project/icots-workshop/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://driftlessdata.space/project/icots-workshop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Web scraping and data visualization with Python and Tableau</title>
      <link>http://driftlessdata.space/project/uscots2017-workshop/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://driftlessdata.space/project/uscots2017-workshop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling the Vitruvian Man</title>
      <link>http://driftlessdata.space/post/vitruvian/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://driftlessdata.space/post/vitruvian/</guid>
      <description>&lt;p&gt;This post describes an activity I developed for Stat 310: Intermediate Statistics. This course is the second course on statistics at Winona State. I like to think of it as our “introduction to modeling” course, and this activity does just that: introduces students to the idea of a statistical model, including model assessment and fitting. The activity actually comes in two parts, administered at different times in the semester. In the first part, I am trying to get students to think about how to assess and compare proposed models using residuals. In the second, students need to fit their own models, and compare performance of fitted models.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/rgco495f3mhy9my/Vitruvian%20man.docx?dl=0&#34;&gt;Link to Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/u8paynlm9gom22m/HW4.docx?dl=0&#34;&gt;Link to Part 2 (Question 3)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;em&gt;Vitruvian Man&lt;/em&gt; is a well-known drawing and study by Leonardo DaVinci:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://driftlessdata.space/img/vitruvian-man.jpg&#34; alt=&#34;Figure Source:  https://en.wikipedia.org/wiki/Vitruvian_Man&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/Vitruvian_Man&#34;&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Vitruvian_Man&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Vitruvian_Man&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This work is sometimes referred to as &lt;em&gt;Canon of Proportions&lt;/em&gt;, and is essentially a series of proposed proportions. My activity focuses on two of these proportions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;the length of the outspread arms is equal to the height of a man&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;the distance from the elbow to the tip of the hand is a quarter of the height of a man&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;part-1-comparing-proposed-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 1: comparing proposed models&lt;/h2&gt;
&lt;p&gt;These proportions proposed by DaVinci are essentially two proposed statistical models! We can test these models by collecting some data. This I did by having the students pair up and measure the following three quantities:&lt;/p&gt;
&lt;p&gt;A. Height;&lt;br /&gt;
B. “Wingspan” (length of the outspread arms);&lt;br /&gt;
C. “Elbow-tip” (the distance from the elbow to the tip of the hand).&lt;/p&gt;
&lt;p&gt;With these three measurements, we can assess which of DaVinci’s proposed proportions is “best!” Notice that his first proportion is like fitting the model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Height = \beta_0 + \beta_1 \times Wingspan + \epsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this model, both the intercept and the slope are fixed with &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The second model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Height = \beta_0 + \beta_1 \times ElbowTip + \epsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again in this model, the model parameters are fixed with &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 = 4\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So which model is better?! This motivates finding the modeled &lt;span class=&#34;math inline&#34;&gt;\(\widehat{Height}\)&lt;/span&gt; given each equation, and comparing the sum of squared residuals, &lt;em&gt;SSError&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;But first, let’s visualize! Here is a scatterplot of Height vs Wingspan using the data collected by the 22 students in my Spring 2018 section of Stat 310. The line indicates the proposed model with &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 = 1\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://driftlessdata.space/post/vitruvian_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model is about perfect for two students; but clearly imperfect for the other 20. What about Elbow-Tip?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://driftlessdata.space/post/vitruvian_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, this fit looks worse. We can quantify this by computing SSError, which equals 156 using wingspan and 504 using Elbow-Tip.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-fitting-simple-linear-regression-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 2: Fitting simple linear regression models&lt;/h2&gt;
&lt;p&gt;So, DaVinci’s proposed model using Wingspan wasn’t horrible, but the proposal using Elbow-Tip was. Can we improve these proposed proportions by fitting simple linear regression models, and if so, which &lt;em&gt;fitted&lt;/em&gt; model is best?&lt;/p&gt;
&lt;p&gt;The figure below shows the actual height plotted versus the fitted heights from the two mdoels, along with the (0,1) line:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://driftlessdata.space/post/vitruvian_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s difficult to tell which model performs best! Here we really do need the SSErrors, which are 117.8 using wingspan and 122.4 using Elbow-Tip. So, close! But Wingspan slightly out-performs Elbow-Tip as a predictor of height. (Of course, these are in-sample SSErrors; a more accurate comparison would cross-validate which we discuss later in the course.)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Public data resources with social justice applications</title>
      <link>http://driftlessdata.space/post/about-social-justice/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 -0500</pubDate>
      
      <guid>http://driftlessdata.space/post/about-social-justice/</guid>
      <description>&lt;p&gt;Yesterday, several students and I traveled to St. Olaf to &lt;a href=&#34;../../talk/stolaf/&#34;&gt;illustrate several uses&lt;/a&gt; of interesting, publicly-available data that can be used to investigate &amp;ldquo;social justice&amp;rdquo; issues.  I&amp;rsquo;ve long been meaning to compile all the data sources and some example projects I&amp;rsquo;ve developed over the years, and this talk provided just the right motivation.   Accordingly, I have &lt;a href=&#34;../../project/social-justice-data/&#34;&gt;compiled a list&lt;/a&gt; of some of my favorite data sources and example projects and student work.  It takes some time to gather, wrangle, and aggregate some of these data sources. Part of my goal is to share some of the work I have already done to clean some of the messier data sets and pare them down to a more ready-to-use format.&lt;/p&gt;

&lt;p&gt;Much thanks goes to Dr. Julie Legler from St. Olaf for reaching out and inviting us to dialogue with her students, and for encouraging me to actually put in the work to compile all these resources!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Great public data sources with social justice applications</title>
      <link>http://driftlessdata.space/talk/stolaf/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 -0500</pubDate>
      
      <guid>http://driftlessdata.space/talk/stolaf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Public data sources with social justice applications</title>
      <link>http://driftlessdata.space/project/social-justice-data/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>http://driftlessdata.space/project/social-justice-data/</guid>
      <description>

&lt;p&gt;Recently, I and some WSU student volunteers gave a &lt;a href=&#34;../../talk/stolaf/&#34;&gt;talk at St. Olaf&lt;/a&gt; on great public data sources I have been using as sources of investigating &amp;ldquo;social justice&amp;rdquo; applications.  Most of the ways I&amp;rsquo;ve used the data have been for class projects or assignments. That talk was an impetus to compile all the resources I&amp;rsquo;ve been using over the years, along with some example class projects using pre-aggregated data.&lt;/p&gt;

&lt;h3 id=&#34;integrated-public-use-microdata-series-ipums&#34;&gt;Integrated Public Use Microdata Series (IPUMS)&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ipums.org/&#34;&gt;IPUMS&lt;/a&gt; is one of my favorite data sources.  The great folks at the &lt;a href=&#34;https://pop.umn.edu/&#34;&gt;Minnesota Population Center&lt;/a&gt; have done fabulous work collecting, harmonizing, and producing data from the &lt;a href=&#34;https://www.census.gov/programs-surveys/acs/&#34;&gt;American Community Survey&lt;/a&gt;, the &lt;a href=&#34;https://www.census.gov/programs-surveys/cps.html&#34;&gt;Current Population Survey&lt;/a&gt;, and the &lt;a href=&#34;https://www.cdc.gov/nchs/nhis/index.htm&#34;&gt;National Health Interview Survey&lt;/a&gt;, among others.  The group at IPUMS-International even &lt;a href=&#34;../../post/winstats/&#34;&gt;collaborated with us on an REU&lt;/a&gt; during Summer 2017.  Once you get the hang of their data query system, you have an enormous wealth of data available to you.  In December 2017, they even released &lt;a href=&#34;https://cran.r-project.org/web/packages/ipumsr/vignettes/ipums.html&#34;&gt;ipumsr&lt;/a&gt;, an R package that facilitates wrangling their data extracts with R.&lt;/p&gt;

&lt;p&gt;As the M in IPUMS indicates, data extracted from IPUMS are microdata.  This means each row is an individual observation, which makes IPUMS data especially interesting.  All sensible IPUMS data analyses require consideration of the PERWT variable, which indicates how many individuals in the population each row represents.  IPUMS has extensive documentation on PERWT (e.g. &lt;a href=&#34;https://usa.ipums.org/usa-action/variables/PERWT#description_section&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://usa.ipums.org/usa/repwt.shtml&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;h4 id=&#34;example-student-projects-using-ipums-data&#34;&gt;Example student projects using IPUMS data&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Intro Stat final project: investigating historic poverty rates, median income, and high-school completion by race using aggregated data from &lt;a href=&#34;https://cps.ipums.org/cps/&#34;&gt;IPUMS CPS&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/2jzl2beycp45xa7/Stat%20210%20Final%20Project-Spring2016.docx?dl=0&#34;&gt;Prompt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/hoce7k8hi7agn1g/ACS-NC.csv?dl=0&#34;&gt;ACS data on North Carolina&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/s1gcroyd9gz8yqv/NC%20Birth.csv?dl=0&#34;&gt;North Carolina births&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/vm11r751pv7f5mr/IPUMS-Historic.csv?dl=0&#34;&gt;Aggregated IPUMS CPS data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/hndpmqukj20ra73/CDC-Historic.csv?dl=0&#34;&gt;Aggregated CDC WONDER data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Data visualization assignment: population shifts, trend in educational attainment gaps

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/qh9t5ac9du0b50t/DT2-F17.docx?dl=0&#34;&gt;Prompt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/5dhykg7t4aa8cqb/IPUMS-Sex-Empstat.csv?dl=0&#34;&gt;Sex-Empstat Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/ftid3df9962i2il/IPUMS-Sex-Age.csv?dl=0&#34;&gt;Sex-Age data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tableau visualization of STEM and gender using &lt;a href=&#34;https://highered.ipums.org/highered/&#34;&gt;IPUMS Higher-Ed&lt;/a&gt; by &lt;a href=&#34;https://www.linkedin.com/in/reagan-buske-945570150/&#34;&gt;Reagan Buske&lt;/a&gt; (DSCI 310 final project; Fall 2017)&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://public.tableau.com/profile/reagan.buske#!/vizhome/STEM_8/Intro&#34;&gt;Link to Tableau Public dashboard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;cdc-wonder&#34;&gt;CDC WONDER&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://wonder.cdc.gov/&#34;&gt;WONDER&lt;/a&gt; database operated by the Centers for Disease Control and Prevention provide a rich query system for analysis of public health data.  All data is aggregated.  If too few counts are available at the requested level of aggregation, WONDER typically suppresses those counts.  The databases I most often use are the &lt;a href=&#34;https://wonder.cdc.gov/natality.html&#34;&gt;Natality&lt;/a&gt; and &lt;a href=&#34;https://wonder.cdc.gov/natality.html&#34;&gt;Mortality&lt;/a&gt; queries.&lt;/p&gt;

&lt;h4 id=&#34;example-student-projects-using-wonder-data&#34;&gt;Example student projects using WONDER data&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Data visuaization assignment: Teenage fertility rates by race, state, year

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/nhp1fsauhgfpj72/DT3-F17.docx?dl=0&#34;&gt;Prompt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/9kb7zxe9p2jb24c/CDC-Teenage-fertility-data-imputed.csv?dl=0&#34;&gt;Aggregated data&lt;/a&gt; (suppressed rates imputed using teenage fertility rates at the Census Division level)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://public.tableau.com/profile/silas.bergen#!/vizhome/TeenPregnancyintheU_S_/Dash&#34;&gt;Example visualization&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Intro stat assignment (correlation/regression): gun deaths and gun ownership

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/9biho5oh17riezc/HW11.docx?dl=0&#34;&gt;Prompt (question 2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/6yfliq1bdgxa7av/Gun-data.csv?dl=0&#34;&gt;Data&lt;/a&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;mn-department-of-education&#34;&gt;MN Department of Education&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;http://w20.education.state.mn.us/MDEAnalytics/Data.jsp&#34;&gt;Minnesota Department of Education&amp;rsquo;s Data Center&lt;/a&gt; has a wealth of data, though not nearly as nice a query system as IPUMS or WONDER.  Queries must often be by year, with separate Excel/CSV files for each year.  Looking at trends for 10 years, then, requires downloading 10 separate data files and aggregating manually.  Not fun! But there are a lot of data sources here for those interested in looking at staffing, ACT scores, school enrollment, etc.  I spent some time aggregating &lt;a href=&#34;http://w20.education.state.mn.us/MDEAnalytics/DataTopic.jsp?TOPICID=133&#34;&gt;data on disciplinary actions&lt;/a&gt; for the Winona Area Public School district.  I&amp;rsquo;ve turned it into a &lt;a href=&#34;https://public.tableau.com/profile/silas.bergen#!/vizhome/WAPS-infographic/Dash&#34;&gt;Tableau visualization&lt;/a&gt; and also used it as an assigment for my intermediate statistics (&amp;ldquo;Stat 2&amp;rdquo;) class.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Intermediate statistics assignment: modeling racial disparity in disciplinary action rate using data from Winona Area Public Schools

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/xxwv8lchnoeqc0b/Exam%201%20%28take%20home%29.docx?dl=0&#34;&gt;Prompt (question 2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dropbox.com/s/5esaj7aq882o643/WAPS-discipline-data.csv?dl=0&#34;&gt;Data&lt;/a&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;police-data-initiative&#34;&gt;Police Data Initiative&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;http://driftlessdata.space/post/policedatachallenge/&#34;&gt;Police Data Initiative&lt;/a&gt; contains data on 911 calls for over 20 different U.S. cities.  The data are relatively clean, but the variables available vary greatly from city-to-city.  A recent group of students from my data visualization class &lt;a href=&#34;../../post/policedatachallenge/&#34;&gt;did very well&lt;/a&gt; visualizing the calls from Seattle.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data set gold mine</title>
      <link>http://driftlessdata.space/post/ufl/</link>
      <pubDate>Wed, 21 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://driftlessdata.space/post/ufl/</guid>
      <description>&lt;p&gt;Quick one here. As a statistics educator I am always on the lookout for interesting, real, digestable data that illustrate important statistical concepts. That’s a tall order!&lt;/p&gt;
&lt;p&gt;One site that I visit again and again is this excellent repository hosted at University of Florida. &lt;a href=&#34;http://www.stat.ufl.edu/~winner/datasets.html&#34;&gt;Here’s the link.&lt;/a&gt; I regularly ping this website for classes ranging from intro stats to experimental design to regression analysis. Not only are they varied in scope and organized by topic, they also have brief descriptions and citations of original sources. It’s a gold mine! Hat tip to my colleague &lt;a href=&#34;http://course1.winona.edu/bdeppa/&#34;&gt;Brant Deppa&lt;/a&gt; aka Data Hound for originally cluing me in to this website.&lt;/p&gt;
&lt;p&gt;Just an example, here’s one on modeling math scores as a function of LSD concentration I recently used in a &lt;a href=&#34;https://www.dropbox.com/s/u8paynlm9gom22m/HW4.docx?dl=0&#34;&gt;homework assignment&lt;/a&gt; for my &lt;a href=&#34;../courses/stat310-home/&#34;&gt;intermediate statistics course&lt;/a&gt; (spoiler alert: taking LSD is &lt;em&gt;not&lt;/em&gt; recommended to improve math test score.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
df &amp;lt;- read.table(&amp;#39;http://www.stat.ufl.edu/~winner/data/lsd.dat&amp;#39;,header=FALSE,col.names=c(&amp;#39;LSD&amp;#39;,&amp;#39;Score&amp;#39;))
ggplot(data = df,aes(x = LSD, y = Score)) + 
  geom_point() + geom_smooth(method=&amp;quot;lm&amp;quot;) + 
  xlab(&amp;#39;LSD concentration (mcg/kg)&amp;#39;) + ylab(&amp;#39;Math score (out of 100)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://driftlessdata.space/post/UFL_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Notes from Liberal Arts Data Science Workshop</title>
      <link>http://driftlessdata.space/post/lads/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 -0600</pubDate>
      
      <guid>http://driftlessdata.space/post/lads/</guid>
      <description>

&lt;p&gt;When temperatures hit 0&amp;deg;F in Minnesota, what better remedy than to head to Florida and talk data science curriculum!  The 2-day workshop was held at the New College of Florida in Sarasota, FL.  This post reflects some of the ideas circulated at the workshop that stood out to me.&lt;/p&gt;

&lt;h3 id=&#34;multivariate-thinking-and-the-introductory-statistics-and-data-science-course-preparing-students-to-make-sense-of-a-world-of-observational-data-nick-horton&#34;&gt;Multivariate thinking and the introductory statistics and data science course: preparing students to make sense of a world of observational data (Nick Horton)&lt;/h3&gt;

&lt;p&gt;In this talk, Dr. Horton emphasized the fact that most data nowadays is &amp;ldquo;found data&amp;rdquo; of the observational nature.  In other words, it is rare to encounter studies that implement careful randomization into groups in order to account for confounding variables.
In light of this, he made the following suggestions.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In introductory statistics classes, focus less on technical assumptions of 2-sample t-test (for example sample size and degrees-of-freedom), and more on issues of confounding and randomization.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Bring multivariate thinking into the course early.  One easy way this can be done is to introduce data visualization from Day 1.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Introductory classes should emphasize writing, projects, and visualization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He also gave several examples of confounding; one can never have too many in their repertoire!  An example I really liked:  if a study finds that people who use sunscreen tend to have higher rates of skin cancer, would this imply that sunscreen is dangerous to use?  The confounding variable in this case would be sun exposure.  Of course, people who are using sunscreen are probably experiencing greater sun exposure, which is a risk factor for skin cancer.&lt;/p&gt;

&lt;h3 id=&#34;projects-first-in-an-interdisciplinary-data-science-curriculum-jessen-havill&#34;&gt;Projects first in an interdisciplinary data science curriculum (Jessen Havill)&lt;/h3&gt;

&lt;p&gt;Dr. Havill gave an overview of the new Data Analytics major at Denison University.  The major is intentionally &lt;em&gt;not&lt;/em&gt; named Data Science to emphasize  the liberal arts nature of the major.  It is extremely cross-disciplinary (the two new upper-level Data Analytics courses are taught by an &lt;a href=&#34;https://denison.edu/people/sarah-supp&#34;&gt;ecologist&lt;/a&gt; and an &lt;a href=&#34;https://denison.edu/people/anthony-bonifonte&#34;&gt;operations researcher&lt;/a&gt;).  It was interesting to hear about the program at Denison and the thought they put into it.  Check out the &lt;a href=&#34;https://denison.edu/academics/data-analytics&#34;&gt;program website&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&#34;computer-science-in-the-data-science-curriculum-panel&#34;&gt;Computer science in the data science curriculum (Panel)&lt;/h3&gt;

&lt;p&gt;This panel included Jessen Havill of Denison University; Dennis F.X. Mathaisel of Babson College; Julie Medero of Harvey Mudd College; and Imad Rahal of St. John&amp;rsquo;s University and The College of St. Benedict.  Some pertinent features of the panel:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;What CS skills are essential for data science?&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The single most important thing, according to Dr. Havill, is &lt;strong&gt;&lt;em&gt;abstraction&lt;/em&gt;&lt;/strong&gt;.  This concept is more important than the argument of whether this language is better than that language, and is something that can be taught in CS courses from Day 1.&lt;/li&gt;
&lt;li&gt;Computational thinking that translates a problem into a computational solution, according to Dr. Medero.&lt;/li&gt;
&lt;li&gt;How to even represent data that comes in nonstandard form, according to Dr. Rahal.  The ability to work with data of large Volume, Velocity, and in a wide Varieties of structure.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;Are more proprietary tools or more general purpose tools more important?&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The ability to learn something new is more important than expertise in a specific tool, according to Dr. Medero.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;We will never keep up with all the proprietary tools.  The languages I want to use are those that are best for teaching.  Choosing a tool because it&amp;rsquo;s hot right now is not necessarily wise, according to Dr. Havill.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;florida-panthers-consulting-projects-brian-macdonald&#34;&gt;Florida Panthers consulting projects (Brian Macdonald)&lt;/h3&gt;

&lt;p&gt;Probably my favorite presentation!  Brian is the Director of Hockey Analytics for the Florida Panthers, transitioning toward DIrector of Data Science and Research for the Panthers.  In this talk, Brian discussed some fascinating projects he&amp;rsquo;s worked on with students pursuing master&amp;rsquo;s degrees in business analytics.&lt;/p&gt;

&lt;p&gt;In the first project, he described a model for predicting attendance for games using only information known before tickets go on sale.  This will help answer questions like, which games should be in which tiers for variable pricing?  What kinds of requests should the team make when the league is developing the schedule?  For example, does it make better sense from a sales standpoint to schedule good teams on a Saturday and a bad team during the week, or vice versa?&lt;/p&gt;

&lt;p&gt;This project used data on announced attendance from nhl.com.  Predictors of attendance included day of week, holiday, month, opponent.  Interesting nuggets: nobody wants to go to games on Halloween or Easter; and people like to go to games against the &amp;ldquo;original 6&amp;rdquo; NHL teams.&lt;/p&gt;

&lt;p&gt;The second project centered on understanding what influences season ticket renewal.  In short, people who attend more high-scoring, close games that their team wins, are more likely to renew.&lt;/p&gt;

&lt;p&gt;Brian also discussed skills he looks for in interns.  He emphasized skills in data management and merging and data visualization more than analysis skills.  Coding experience is non-negotiable.&lt;/p&gt;

&lt;h3 id=&#34;and-of-course&#34;&gt;And, of course&amp;hellip;&lt;/h3&gt;

&lt;p&gt;&amp;hellip;there was beach time.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
 &lt;figure&gt;
 &lt;img src=&#34;http://driftlessdata.space/img/beach2.jpg&#34; width=&#34;750&#34;&gt;
 &lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
